train=[('사과가 좋아','pos'),
      ('밤에 먹는 사과는 비추야','neg'),
       ('사과가 잘 익었어 맛있겠다','pos'),
       ('바나나는 맛있어','pos'),
       ('수박은 별로야','neg')
      ]
      
train








# 모든 단어 집합 만들기

from konlpy.tag import Okt

okt=Okt()

def pos_tokenize(raw_sent):
    pos_sent=[]
    # raw_sent: 사과가 좋아
    sent=okt.pos(raw_sent, norm=True, stem=True) # norm ㅋㅋㅋ를 정규화 stem=원형
    # sent:[('사과','명사'),('가','조사'),('좋다','형용사')]
    
    for tup in sent:
        word,tag=tup[0],tup[1]
        word_tag=word+'/'+tag
        pos_sent.append(word_tag)
    return ' '.join(pos_sent)
    


from nltk.tokenize import word_tokenize #자연어 처리 및 텍스트 분석을 위한 패키지
import nltk

all_words=set()

for tup in train:
    sent, label=tup[0], tup[1]
    sent=pos_tokenize(sent) # pos_tokenize 함수 추가 => part of speech
    words = word_tokenize(sent)
    for word in words:
        all_words.add(word)
        
        
print(all_words)







train_features=[]

for tup in train:
    sent,label=tup[0],tup[1] # 단어 및 라벨링 된 각 튜플들 불러오기
    sent=pos_tokenize(sent)
    words=word_tokenize(sent) # 단어들을 토큰으로 만든다
    tmp={set_word : (set_word in words) for set_word in all_words} # 딕셔너리 형태 키는 모든 단어


    sent_tup=(tmp,label)        # 
    train_features.append(sent_tup) # 
    
    
print(train_features[0])
print(train_features[1])
print(train_features[2])
print(train_features[3])






classifier=nltk.NaiveBayesClassifier.train(train_features)
classifier.show_most_informative_features()










test_sent='사과는 별로야'

test_sent=pos_tokenize(test_sent)
words=word_tokenize(test_sent)
test_feature={set_word: (set_word in words) for set_word in all_words}

print(test_feature)








classifier.classify(test_feature)




































































