train=[('사과가 좋아','pos'),
      ('밤에 먹는 사과는 비추야','neg'),
       ('사과가 잘 익었어 맛있겠다','pos'),
       ('바나나는 맛있어','pos')
      ]

train



from nltk.tokenize import word_tokenize #자연어 처리 및 텍스트 분석을 위한 패키지
import nltk

nltk.download('punkt')



train=[('I like you','pos'),('I hate you','neg'), 
      ('I enjoyed it','pos'),('I hate it','neg')] # 튜플 형태로 입력
train




# 나이브 베이즈 분류 방식을 이용할것이다.

# 모든 단어 집합 만들기
# 각 문장을 단어가 있는지 없는지 여부로 표현
# 단어별 확률 계산

#1. 단어 집합 만들기

all_words=set()

for tup in train:
    sent,label=tup[0],tup[1]
    words=word_tokenize(sent)
    for word in words:
        all_words.add(word)
        
all_words






# 각 문장을 단어가 있는지 없느지 여부로 표현

# 단어가 있는지 없는지로 판단 후 빈도만을 이용


train_features=[]

for tup in train:
    sent,label=tup[0],tup[1] # 단어 및 라벨링 된 각 튜플들 불러오기
    words=word_tokenize(sent) # 단어들을 토큰으로 만든다
    tmp=dict()                # 딕셔너리 형태 키는 모든 단어
    for set_word in all_words: # 위에 각 형태소 단위 단어들이
        if set_word in words:  # 여기서 다시금 쪼개진 단어들 목록에 있다면
            tmp[set_word]=True # 딕셔너리 형태에서 True를 표시함
        else:
            tmp[set_word]=False # 없다면 False 표시
    sent_tup=(tmp,label)        # 
    train_features.append(sent_tup) # 
    
    
print(train_features[0])
print(train_features[1])
print(train_features[2])
print(train_features[3])






# 간단 1
train_features=[]

for tup in train:
    sent,label=tup[0],tup[1] # 단어 및 라벨링 된 각 튜플들 불러오기
    words=word_tokenize(sent) # 단어들을 토큰으로 만든다
    tmp=dict()                # 딕셔너리 형태 키는 모든 단어
    for set_word in all_words: # 위에 각 형태소 단위 단어들이
        tmp[set_word]=(set_word in words) # 좀더 간단한 방법

    sent_tup=(tmp,label)        # 
    train_features.append(sent_tup) # 
    
    
print(train_features[0])
print(train_features[1])
print(train_features[2])
print(train_features[3])






# 간단 2

train_features=[]

for tup in train:
    sent,label=tup[0],tup[1] # 단어 및 라벨링 된 각 튜플들 불러오기
    words=word_tokenize(sent) # 단어들을 토큰으로 만든다
    tmp={set_word : (set_word in words) for set_word in all_words} # 딕셔너리 형태 키는 모든 단어


    sent_tup=(tmp,label)        # 
    train_features.append(sent_tup) # 
    
    
print(train_features[0])
print(train_features[1])
print(train_features[2])
print(train_features[3])






# 단어별 확률 계산

classifier=nltk.NaiveBayesClassifier.train(train_features)
classifier.show_most_informative_features()





test_sent='I like it'

words=word_tokenize(test_sent)
test_feature={set_word: (set_word in words) for set_word in all_words}

print(test_feature)





classifier.classify(test_feature)





# 라플라스 스무딩 => 분자나 분모에 0이 들어가지 않게 하여, 에러를 방지

































































































